{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea89806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "==================================================\n",
      "Best Parameters:  {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8927038626609443\n",
      "Precision: 0.8432835820895522\n",
      "Recall: 0.9658119658119658\n",
      "F1 Score: 0.9003984063745021\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8884120171673819\n",
      "Precision: 0.8823529411764706\n",
      "Recall: 0.8974358974358975\n",
      "F1 Score: 0.8898305084745762\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9313304721030042\n",
      "Precision: 0.9464285714285714\n",
      "Recall: 0.9137931034482759\n",
      "F1 Score: 0.9298245614035087\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.7811158798283262\n",
      "Precision: 0.9113924050632911\n",
      "Recall: 0.6206896551724138\n",
      "F1 Score: 0.7384615384615385\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.853448275862069\n",
      "Precision: 0.9880952380952381\n",
      "Recall: 0.7155172413793104\n",
      "F1 Score: 0.83\n",
      "\n",
      "Average Accuracy: 0.869402101524345\n",
      "Average Precision: 0.9143105475706246\n",
      "Average Recall: 0.8226495726495726\n",
      "Average F1 Score: 0.8577030029428251\n",
      "\n",
      "Model: Logistic Regression\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.64598934        nan 0.64598934        nan 0.65371467]\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 10, 'penalty': 'l2'}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.6437768240343348\n",
      "Precision: 0.6370967741935484\n",
      "Recall: 0.6752136752136753\n",
      "F1 Score: 0.6556016597510375\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.7553648068669528\n",
      "Precision: 0.7941176470588235\n",
      "Recall: 0.6923076923076923\n",
      "F1 Score: 0.7397260273972601\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.7553648068669528\n",
      "Precision: 0.8105263157894737\n",
      "Recall: 0.6637931034482759\n",
      "F1 Score: 0.7298578199052133\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.5364806866952789\n",
      "Precision: 0.5606060606060606\n",
      "Recall: 0.31896551724137934\n",
      "F1 Score: 0.4065934065934066\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.5775862068965517\n",
      "Precision: 0.618421052631579\n",
      "Recall: 0.4051724137931034\n",
      "F1 Score: 0.48958333333333337\n",
      "\n",
      "Average Accuracy: 0.6537146662720141\n",
      "Average Precision: 0.6841535700558969\n",
      "Average Recall: 0.5510904804008252\n",
      "Average F1 Score: 0.6042724493960502\n",
      "\n",
      "Model: Naïve Bayes\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.5278969957081545\n",
      "Precision: 0.5636363636363636\n",
      "Recall: 0.26495726495726496\n",
      "F1 Score: 0.3604651162790697\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.51931330472103\n",
      "Precision: 0.5111111111111111\n",
      "Recall: 0.9829059829059829\n",
      "F1 Score: 0.6725146198830408\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.51931330472103\n",
      "Precision: 0.5092592592592593\n",
      "Recall: 0.9482758620689655\n",
      "F1 Score: 0.6626506024096386\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.48497854077253216\n",
      "Precision: 0.4898989898989899\n",
      "Recall: 0.8362068965517241\n",
      "F1 Score: 0.6178343949044586\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.5129310344827587\n",
      "Precision: 0.5066666666666667\n",
      "Recall: 0.9827586206896551\n",
      "F1 Score: 0.6686217008797655\n",
      "\n",
      "Average Accuracy: 0.512886636081101\n",
      "Average Precision: 0.5161144781144781\n",
      "Average Recall: 0.8030209254347185\n",
      "Average F1 Score: 0.5964172868711947\n",
      "\n",
      "Model: SVM\n",
      "==================================================\n",
      "Best Parameters:  {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8497854077253219\n",
      "Precision: 0.7697368421052632\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8698884758364311\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.8125\n",
      "Recall: 1.0\n",
      "F1 Score: 0.896551724137931\n",
      "\n",
      "Average Accuracy: 0.866952789699571\n",
      "Average Precision: 0.7911184210526319\n",
      "Average Recall: 1.0\n",
      "Average F1 Score: 0.8832200999871809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Read the data into a dataframe\n",
    "df = pd.read_csv('cleaned_engagement_data.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df[['total_direct_mentions', \n",
    "        'total_indirect_mentions', \n",
    "        'total_likes', \n",
    "        'total_retweets', \n",
    "        'total_project_followers', \n",
    "        'total_indirect_followers', \n",
    "        'soft_cap']]\n",
    "y = df['ico_success']\n",
    "\n",
    "# Perform Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Define the parameter grids for grid search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "nb_param_grid = {}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), rf_param_grid),\n",
    "    'Logistic Regression': (LogisticRegression(random_state=42), lr_param_grid),\n",
    "    'Naïve Bayes': (GaussianNB(), nb_param_grid),\n",
    "    'SVM': (SVC(random_state=42), svm_param_grid)\n",
    "}\n",
    "\n",
    "# Perform grid search and cross-validation for each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    # Perform 5-fold cross-validation with the best model\n",
    "    cv_results = cross_validate(grid_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    overfitted_folds = 0  # Counter for overfitted folds\n",
    "    for fold_idx, fold_result in enumerate(cv_results['test_accuracy']):\n",
    "        if fold_result == 1.0:  # Check for overfitted fold\n",
    "            overfitted_folds += 1\n",
    "            continue  # Skip overfitted fold\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"Accuracy: {fold_result}\")\n",
    "        print(f\"Precision: {cv_results['test_precision'][fold_idx]}\")\n",
    "        print(f\"Recall: {cv_results['test_recall'][fold_idx]}\")\n",
    "        print(f\"F1 Score: {cv_results['test_f1'][fold_idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate average results across non-overfitted folds\n",
    "    num_folds = len(cv_results['test_accuracy'])\n",
    "    num_non_overfitted_folds = num_folds - overfitted_folds\n",
    "    avg_accuracy = (sum(cv_results['test_accuracy']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_precision = (sum(cv_results['test_precision']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_recall = (sum(cv_results['test_recall']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_f1 = (sum(cv_results['test_f1']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "\n",
    "    # Print the average results\n",
    "    print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1 Score: {avg_f1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564bdccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBboost\n",
      "==================================================\n",
      "Best Parameters:  {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200, 'reg_lambda': 5}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.9141630901287554\n",
      "Precision: 0.875968992248062\n",
      "Recall: 0.9658119658119658\n",
      "F1 Score: 0.9186991869918699\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9012875536480687\n",
      "Precision: 0.9051724137931034\n",
      "Recall: 0.8974358974358975\n",
      "F1 Score: 0.9012875536480686\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.944206008583691\n",
      "Precision: 1.0\n",
      "Recall: 0.8879310344827587\n",
      "F1 Score: 0.9406392694063928\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8283261802575107\n",
      "Precision: 1.0\n",
      "Recall: 0.6551724137931034\n",
      "F1 Score: 0.7916666666666666\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.853448275862069\n",
      "Precision: 1.0\n",
      "Recall: 0.7068965517241379\n",
      "F1 Score: 0.8282828282828283\n",
      "\n",
      "Average Accuracy: 0.888286221696019\n",
      "Average Precision: 0.956228281208233\n",
      "Average Recall: 0.8226495726495726\n",
      "Average F1 Score: 0.8761151009991652\n",
      "\n",
      "Model: CatBoost\n",
      "==================================================\n",
      "Best Parameters:  {'depth': 10, 'iterations': 300, 'learning_rate': 0.03}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.9184549356223176\n",
      "Precision: 0.8712121212121212\n",
      "Recall: 0.9829059829059829\n",
      "F1 Score: 0.9236947791164658\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9055793991416309\n",
      "Precision: 0.8861788617886179\n",
      "Recall: 0.9316239316239316\n",
      "F1 Score: 0.9083333333333333\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9828326180257511\n",
      "Precision: 1.0\n",
      "Recall: 0.9655172413793104\n",
      "F1 Score: 0.9824561403508771\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8497854077253219\n",
      "Precision: 1.0\n",
      "Recall: 0.6982758620689655\n",
      "F1 Score: 0.8223350253807107\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8879310344827587\n",
      "Precision: 1.0\n",
      "Recall: 0.7758620689655172\n",
      "F1 Score: 0.8737864077669902\n",
      "\n",
      "Average Accuracy: 0.908916678999556\n",
      "Average Precision: 0.9514781966001479\n",
      "Average Recall: 0.8708370173887415\n",
      "Average F1 Score: 0.9021211371896755\n",
      "\n",
      "Model: Random Forest\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.9184549356223176\n",
      "Precision: 0.8828125\n",
      "Recall: 0.9658119658119658\n",
      "F1 Score: 0.9224489795918367\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9055793991416309\n",
      "Precision: 0.905982905982906\n",
      "Recall: 0.905982905982906\n",
      "F1 Score: 0.905982905982906\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9484978540772532\n",
      "Precision: 1.0\n",
      "Recall: 0.896551724137931\n",
      "F1 Score: 0.9454545454545454\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.7939914163090128\n",
      "Precision: 1.0\n",
      "Recall: 0.5862068965517241\n",
      "F1 Score: 0.7391304347826086\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.853448275862069\n",
      "Precision: 1.0\n",
      "Recall: 0.7068965517241379\n",
      "F1 Score: 0.8282828282828283\n",
      "\n",
      "Average Accuracy: 0.8839943762024568\n",
      "Average Precision: 0.9577590811965813\n",
      "Average Recall: 0.812290008841733\n",
      "Average F1 Score: 0.8682599388189448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "models = {\n",
    "    'XGBboost': XGBClassifier(random_state=42, learning_rate=0.03, max_depth=3, n_estimators=300, reg_lambda=2),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, iterations=200, depth=4, loss_function='Logloss',\n",
    "                                   l2_leaf_reg=1e-20, leaf_estimation_iterations=10, logging_level='Silent',\n",
    "                                   learning_rate=0.03),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Define the parameter grids for grid search\n",
    "param_grids = {\n",
    "    'XGBboost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.03, 0.1],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'reg_lambda': [1, 2, 5]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.03, 0.1],\n",
    "        'depth': [3, 5, 10]\n",
    "    },\n",
    "    'Random Forest': {},\n",
    "}\n",
    "    \n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get the parameter grid for the current model\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    print()\n",
    "    \n",
    "    # Perform 5-fold cross-validation with the best model\n",
    "    cv_results = cross_validate(grid_search.best_estimator_, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    overfitted_folds = 0  # Counter for overfitted folds\n",
    "    for fold_idx, fold_result in enumerate(cv_results['test_accuracy']):\n",
    "        if fold_result == 1.0:  # Check for overfitted fold\n",
    "            overfitted_folds += 1\n",
    "            continue  # Skip overfitted fold\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"Accuracy: {fold_result}\")\n",
    "        print(f\"Precision: {cv_results['test_precision'][fold_idx]}\")\n",
    "        print(f\"Recall: {cv_results['test_recall'][fold_idx]}\")\n",
    "        print(f\"F1 Score: {cv_results['test_f1'][fold_idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate average results across non-overfitted folds\n",
    "    num_folds = len(cv_results['test_accuracy'])\n",
    "    num_non_overfitted_folds = num_folds - overfitted_folds\n",
    "    avg_accuracy = (sum(cv_results['test_accuracy']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_precision = (sum(cv_results['test_precision']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_recall = (sum(cv_results['test_recall']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "    avg_f1 = (sum(cv_results['test_f1']) - (overfitted_folds*1)) / num_non_overfitted_folds\n",
    "\n",
    "    # Print the average results\n",
    "    print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1 Score: {avg_f1}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
