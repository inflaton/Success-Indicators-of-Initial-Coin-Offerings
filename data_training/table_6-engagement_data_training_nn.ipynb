{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cdf1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data into a dataframe\n",
    "df = pd.read_csv(\"cleaned_engagement_data.csv\")\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df[\n",
    "    [\n",
    "        \"total_direct_mentions\",\n",
    "        \"total_indirect_mentions\",\n",
    "        \"total_likes\",\n",
    "        \"total_retweets\",\n",
    "        \"total_project_followers\",\n",
    "        \"total_indirect_followers\",\n",
    "        \"soft_cap\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"ico_success\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d04a5",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903d797",
   "metadata": {},
   "source": [
    "To get started, we use a very simple classification problem and a very simple multi-layer perceptron architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa2fcd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ef1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.helper import SkorchDoctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a94d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab48b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea515f93",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c39fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train.to_numpy().astype(np.float32), y_train.to_numpy().astype(np.int64)\n",
    "X_test, y_test = X_test.to_numpy().astype(np.float32), y_test.to_numpy().astype(\n",
    "    np.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4eccba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((652, 7), (652,), 0.7285276073619632)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f96e0",
   "metadata": {},
   "source": [
    "### Definition of the `PyTorch` classification `module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec30b3a",
   "metadata": {},
   "source": [
    "This is just an MLP with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab2fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=7,\n",
    "        num_units=1024,\n",
    "        n_classes=2,\n",
    "        nonlin=F.relu,\n",
    "        dropout=0.1,\n",
    "        depth=2,\n",
    "        batchnorm=True,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_units = num_units\n",
    "        self.n_classes = n_classes\n",
    "        self.nonlin = nonlin\n",
    "        self.batchnorm = batchnorm\n",
    "        self.depth = depth\n",
    "\n",
    "        self.dense0 = nn.Linear(self.num_features, self.num_units)\n",
    "        self.nonlin = self.nonlin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(1, self.depth):\n",
    "            layers.append(nn.Linear(self.num_units, self.num_units))\n",
    "        self.dense1 = nn.Sequential(*layers)\n",
    "\n",
    "        self.output = nn.Linear(self.num_units, self.n_classes)\n",
    "        self.bn = nn.BatchNorm1d(self.n_classes)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "\n",
    "        if self.batchnorm:\n",
    "            X = self.bn(X)\n",
    "\n",
    "        X = F.softmax(X, dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3253b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")  # use gpu\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f24c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"batch_size\": 20,\n",
    "    \"max_epochs\": 10,\n",
    "    \"module__depth\": 6,\n",
    "    \"module__dropout\": 0.4,\n",
    "    \"module__num_units\": 66,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22344fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_dir: ./checkpoints exists\n",
      "deleting dir: ./checkpoints/cp1\n",
      "deleting dir: ./checkpoints/cp2\n",
      "deleting dir: ./checkpoints/cp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoints_dir = \"./checkpoints\"\n",
    "path = Path(checkpoints_dir)\n",
    "\n",
    "if path.exists():\n",
    "    print(f\"checkpoints_dir: {checkpoints_dir} exists\")\n",
    "    for root, dirs, files in os.walk(checkpoints_dir):\n",
    "        for file in files:\n",
    "            checkpoint = f\"{root}/{file}\"\n",
    "            print(f\"deleting file: {checkpoint}\")\n",
    "            os.unlink(checkpoint)\n",
    "        for dir in dirs:\n",
    "            checkpoint = f\"{root}/{dir}\"\n",
    "            print(f\"deleting dir: {checkpoint}\")\n",
    "            shutil.rmtree(checkpoint)\n",
    "else:\n",
    "    print(f\"checkpoints_dir: {checkpoints_dir} doesn't exist. creating it ...\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19da2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4391a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "cp = Checkpoint(dirname=f\"{checkpoints_dir}/cp1\")\n",
    "\n",
    "optimal = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=6,\n",
    "    module__num_units=66,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp] = optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297aa9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.9041\u001b[0m       \u001b[32m0.2578\u001b[0m        \u001b[35m0.7963\u001b[0m     +  0.1596\n",
      "      2        \u001b[36m0.7319\u001b[0m       \u001b[32m0.2969\u001b[0m        \u001b[35m0.7785\u001b[0m     +  0.0464\n",
      "      3        \u001b[36m0.7088\u001b[0m       \u001b[32m0.5312\u001b[0m        \u001b[35m0.6882\u001b[0m     +  0.0473\n",
      "      4        \u001b[36m0.7076\u001b[0m       \u001b[32m0.6406\u001b[0m        \u001b[35m0.6692\u001b[0m     +  0.0497\n",
      "      5        \u001b[36m0.6969\u001b[0m       \u001b[32m0.6953\u001b[0m        0.6698        0.0489\n",
      "      6        \u001b[36m0.6869\u001b[0m       \u001b[32m0.7031\u001b[0m        \u001b[35m0.6613\u001b[0m     +  0.0492\n",
      "      7        \u001b[36m0.6810\u001b[0m       \u001b[32m0.7188\u001b[0m        0.6674        0.0571\n",
      "      8        0.6890       0.7188        0.6826        0.0445\n",
      "      9        \u001b[36m0.6755\u001b[0m       0.7188        0.7006        0.0466\n",
      "     10        0.6803       \u001b[32m0.7422\u001b[0m        0.6628        0.0460\n",
      "CPU times: user 894 ms, sys: 137 ms, total: 1.03 s\n",
      "Wall time: 1.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dense0): Linear(in_features=7, out_features=66, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (dense1): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (1): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (2): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (3): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (4): Linear(in_features=66, out_features=66, bias=True)\n",
       "    )\n",
       "    (output): Linear(in_features=66, out_features=2, bias=True)\n",
       "    (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimal.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9c3a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LoadInitState\n",
    "\n",
    "# load_state = LoadInitState(cp)\n",
    "cp2 = Checkpoint(dirname=f\"{checkpoints_dir}/cp2\")\n",
    "\n",
    "optimal2 = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=7,\n",
    "    module__num_units=66,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp2],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp2] = optimal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c9d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7735\u001b[0m       \u001b[32m0.6328\u001b[0m        \u001b[35m0.6499\u001b[0m     +  0.0553\n",
      "      2        0.7787       0.6328        \u001b[35m0.6485\u001b[0m     +  0.0494\n",
      "      3        \u001b[36m0.7332\u001b[0m       \u001b[32m0.7266\u001b[0m        0.7040        0.0537\n",
      "      4        \u001b[36m0.7080\u001b[0m       \u001b[32m0.7578\u001b[0m        0.6541        0.0517\n",
      "      5        \u001b[36m0.6854\u001b[0m       0.7422        0.6716        0.0531\n",
      "      6        0.6942       0.7266        0.7321        0.0490\n",
      "      7        \u001b[36m0.6797\u001b[0m       0.7266        0.6976        0.0482\n",
      "      8        \u001b[36m0.6742\u001b[0m       0.7422        0.6642        0.0458\n",
      "      9        \u001b[36m0.6625\u001b[0m       0.7344        0.6539        0.0520\n",
      "     10        0.6732       0.7266        0.7404        0.0508\n",
      "CPU times: user 468 ms, sys: 82 ms, total: 550 ms\n",
      "Wall time: 536 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = optimal2.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9fe2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_state = LoadInitState(cp2)\n",
    "cp3 = Checkpoint(dirname=f\"{checkpoints_dir}/cp3\")\n",
    "\n",
    "optimal3 = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=6,\n",
    "    module__num_units=80,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp3],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp3] = optimal3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "785a099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7625\u001b[0m       \u001b[32m0.7266\u001b[0m        \u001b[35m0.6981\u001b[0m     +  0.0518\n",
      "      2        \u001b[36m0.6957\u001b[0m       0.7266        0.7133        0.0498\n",
      "      3        \u001b[36m0.6952\u001b[0m       0.7266        0.7275        0.0495\n",
      "      4        \u001b[36m0.6930\u001b[0m       0.7266        \u001b[35m0.6954\u001b[0m     +  0.0525\n",
      "      5        0.6944       0.6328        0.7495        0.0535\n",
      "      6        \u001b[36m0.6860\u001b[0m       0.7266        0.7094        0.0488\n",
      "      7        0.6862       0.7266        \u001b[35m0.6757\u001b[0m     +  0.0495\n",
      "      8        \u001b[36m0.6766\u001b[0m       \u001b[32m0.7422\u001b[0m        \u001b[35m0.6673\u001b[0m     +  0.0470\n",
      "      9        \u001b[36m0.6735\u001b[0m       0.7266        0.6758        0.0498\n",
      "     10        \u001b[36m0.6681\u001b[0m       0.7266        0.6845        0.0514\n",
      "CPU times: user 437 ms, sys: 109 ms, total: 546 ms\n",
      "Wall time: 534 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = optimal3.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b46e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "CPU times: user 35.8 ms, sys: 0 ns, total: 35.8 ms\n",
      "Wall time: 34.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8235294117647058, './checkpoints/cp2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "checkpoints = [cp, cp2, cp3]\n",
    "\n",
    "best_f1 = 0\n",
    "best_y_pred = None\n",
    "best_net = None\n",
    "\n",
    "for checkpoint in models:\n",
    "    net = models[checkpoint]\n",
    "    net.initialize()\n",
    "    net.load_params(checkpoint=checkpoint)\n",
    "\n",
    "    y_pred = net.predict(X_test)\n",
    "    f_measure = f1_score(y_test, y_pred)\n",
    "    if f_measure > best_f1:\n",
    "        best_f1 = f_measure\n",
    "        best_net = net\n",
    "        best_cp = checkpoint\n",
    "\n",
    "best_f1, best_cp.dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a664a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 ms, sys: 0 ns, total: 6.95 ms\n",
      "Wall time: 6.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = best_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258c05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"Model\": [\"Na誰ve Bayes\", \"SVM\", \"Logistic Regression\", \"Random Forest\"],\n",
    "    \"Accuracy\": [\n",
    "        0.6463414634146342,\n",
    "        0.6524390243902439,\n",
    "        0.6524390243902439,\n",
    "        0.7682926829268293,\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        0.6540880503144654,\n",
    "        0.6524390243902439,\n",
    "        0.8289473684210527,\n",
    "        0.7804878048780488,\n",
    "    ],\n",
    "    \"Recall\": [0.9719626168224299, 1.0, 0.5887850467289719, 0.897196261682243],\n",
    "    \"F1 Score\": [\n",
    "        0.7819548872180451,\n",
    "        0.7896678966789668,\n",
    "        0.6885245901639344,\n",
    "        0.8347826086956521,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8635cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7621951219512195\n",
      "Precision: 0.7982456140350878\n",
      "Recall: 0.8504672897196262\n",
      "F-measure: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "model_name = \"Neural Network\"\n",
    "result[\"Model\"].append(model_name)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f_measure = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)\n",
    "\n",
    "result[\"Accuracy\"].append(accuracy)\n",
    "result[\"Precision\"].append(precision)\n",
    "result[\"Recall\"].append(recall)\n",
    "result[\"F1 Score\"].append(f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "defd70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Na誰ve Bayes',\n",
       "  'SVM',\n",
       "  'Logistic Regression',\n",
       "  'Random Forest',\n",
       "  'Neural Network'],\n",
       " 'Accuracy': [0.6463414634146342,\n",
       "  0.6524390243902439,\n",
       "  0.6524390243902439,\n",
       "  0.7682926829268293,\n",
       "  0.7621951219512195],\n",
       " 'Precision': [0.6540880503144654,\n",
       "  0.6524390243902439,\n",
       "  0.8289473684210527,\n",
       "  0.7804878048780488,\n",
       "  0.7982456140350878],\n",
       " 'Recall': [0.9719626168224299,\n",
       "  1.0,\n",
       "  0.5887850467289719,\n",
       "  0.897196261682243,\n",
       "  0.8504672897196262],\n",
       " 'F1 Score': [0.7819548872180451,\n",
       "  0.7896678966789668,\n",
       "  0.6885245901639344,\n",
       "  0.8347826086956521,\n",
       "  0.8235294117647058]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eea385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/ico/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Na誰ve Bayes</th>\n",
       "      <td>64.6%</td>\n",
       "      <td>65.4%</td>\n",
       "      <td>97.2%</td>\n",
       "      <td>78.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>65.2%</td>\n",
       "      <td>65.2%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>79.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>65.2%</td>\n",
       "      <td>82.9%</td>\n",
       "      <td>58.9%</td>\n",
       "      <td>68.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>76.8%</td>\n",
       "      <td>78.0%</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>83.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>76.2%</td>\n",
       "      <td>79.8%</td>\n",
       "      <td>85.0%</td>\n",
       "      <td>82.4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy Precision  Recall F1 Score\n",
       "Model                                                  \n",
       "Na誰ve Bayes            64.6%     65.4%   97.2%    78.2%\n",
       "SVM                    65.2%     65.2%  100.0%    79.0%\n",
       "Logistic Regression    65.2%     82.9%   58.9%    68.9%\n",
       "Random Forest          76.8%     78.0%   89.7%    83.5%\n",
       "Neural Network         76.2%     79.8%   85.0%    82.4%"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "perf_ds2 = Dataset.from_dict(result).to_pandas()\n",
    "res2 = perf_ds2.set_index(perf_ds2.columns[0]).mul(100)\n",
    "for key in res2.select_dtypes(include=[\"number\"]).columns:\n",
    "    res2[key] = res2[key].apply(\"{:.1f}%\".format)\n",
    "\n",
    "res2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
