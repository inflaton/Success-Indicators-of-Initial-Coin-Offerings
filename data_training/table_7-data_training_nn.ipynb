{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cdf1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data into a dataframe\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df[\n",
    "    [\n",
    "        \"total_direct_mentions\",\n",
    "        \"total_indirect_mentions\",\n",
    "        \"total_likes\",\n",
    "        \"total_retweets\",\n",
    "        \"total_project_followers\",\n",
    "        \"total_indirect_followers\",\n",
    "        \"total_positive_direct_mentions\",\n",
    "        \"total_negative_direct_mentions\",\n",
    "        \"total_positive_indirect_mentions\",\n",
    "        \"total_negative_indirect_mentions\",\n",
    "        \"soft_cap\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"ico_success\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b3f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 816 entries, 0 to 815\n",
      "Data columns (total 11 columns):\n",
      " #   Column                            Non-Null Count  Dtype\n",
      "---  ------                            --------------  -----\n",
      " 0   total_direct_mentions             816 non-null    int64\n",
      " 1   total_indirect_mentions           816 non-null    int64\n",
      " 2   total_likes                       816 non-null    int64\n",
      " 3   total_retweets                    816 non-null    int64\n",
      " 4   total_project_followers           816 non-null    int64\n",
      " 5   total_indirect_followers          816 non-null    int64\n",
      " 6   total_positive_direct_mentions    816 non-null    int64\n",
      " 7   total_negative_direct_mentions    816 non-null    int64\n",
      " 8   total_positive_indirect_mentions  816 non-null    int64\n",
      " 9   total_negative_indirect_mentions  816 non-null    int64\n",
      " 10  soft_cap                          816 non-null    int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 70.3 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d04a5",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903d797",
   "metadata": {},
   "source": [
    "To get started, we use a very simple classification problem and a very simple multi-layer perceptron architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa2fcd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ef1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.helper import SkorchDoctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a94d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab48b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea515f93",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c39fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train.to_numpy().astype(np.float32), y_train.to_numpy().astype(np.int64)\n",
    "X_test, y_test = X_test.to_numpy().astype(np.float32), y_test.to_numpy().astype(\n",
    "    np.int64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4eccba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((652, 11), (652,), 0.7285276073619632)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f96e0",
   "metadata": {},
   "source": [
    "### Definition of the `PyTorch` classification `module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec30b3a",
   "metadata": {},
   "source": [
    "This is just an MLP with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab2fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features=11,\n",
    "        num_units=1024,\n",
    "        n_classes=2,\n",
    "        nonlin=F.relu,\n",
    "        dropout=0.1,\n",
    "        depth=2,\n",
    "        batchnorm=True,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_units = num_units\n",
    "        self.n_classes = n_classes\n",
    "        self.nonlin = nonlin\n",
    "        self.batchnorm = batchnorm\n",
    "        self.depth = depth\n",
    "\n",
    "        self.dense0 = nn.Linear(self.num_features, self.num_units)\n",
    "        self.nonlin = self.nonlin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(1, self.depth):\n",
    "            layers.append(nn.Linear(self.num_units, self.num_units))\n",
    "        self.dense1 = nn.Sequential(*layers)\n",
    "\n",
    "        self.output = nn.Linear(self.num_units, self.n_classes)\n",
    "        self.bn = nn.BatchNorm1d(self.n_classes)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "\n",
    "        if self.batchnorm:\n",
    "            X = self.bn(X)\n",
    "\n",
    "        X = F.softmax(X, dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3253b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")  # use gpu\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f24c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"batch_size\": 20,\n",
    "    \"max_epochs\": 10,\n",
    "    \"module__depth\": 6,\n",
    "    \"module__dropout\": 0.4,\n",
    "    \"module__num_units\": 66,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22344fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_dir: ./checkpoints exists\n",
      "deleting dir: ./checkpoints/cp1\n",
      "deleting dir: ./checkpoints/cp2\n",
      "deleting dir: ./checkpoints/cp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoints_dir = \"./checkpoints\"\n",
    "path = Path(checkpoints_dir)\n",
    "\n",
    "if path.exists():\n",
    "    print(f\"checkpoints_dir: {checkpoints_dir} exists\")\n",
    "    for root, dirs, files in os.walk(checkpoints_dir):\n",
    "        for file in files:\n",
    "            checkpoint = f\"{root}/{file}\"\n",
    "            print(f\"deleting file: {checkpoint}\")\n",
    "            os.unlink(checkpoint)\n",
    "        for dir in dirs:\n",
    "            checkpoint = f\"{root}/{dir}\"\n",
    "            print(f\"deleting dir: {checkpoint}\")\n",
    "            shutil.rmtree(checkpoint)\n",
    "else:\n",
    "    print(f\"checkpoints_dir: {checkpoints_dir} doesn't exist. creating it ...\")\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19da2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4391a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "cp = Checkpoint(dirname=f\"{checkpoints_dir}/cp1\")\n",
    "\n",
    "optimal = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=6,\n",
    "    module__num_units=66,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp] = optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297aa9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7773\u001b[0m       \u001b[32m0.5938\u001b[0m        \u001b[35m0.6842\u001b[0m     +  0.1405\n",
      "      2        \u001b[36m0.7016\u001b[0m       \u001b[32m0.6172\u001b[0m        \u001b[35m0.6762\u001b[0m     +  0.0466\n",
      "      3        \u001b[36m0.6943\u001b[0m       \u001b[32m0.6250\u001b[0m        0.6814        0.0486\n",
      "      4        0.6954       0.4062        0.6893        0.0507\n",
      "      5        \u001b[36m0.6801\u001b[0m       \u001b[32m0.6406\u001b[0m        \u001b[35m0.6761\u001b[0m     +  0.0459\n",
      "      6        0.6894       \u001b[32m0.7109\u001b[0m        \u001b[35m0.6691\u001b[0m     +  0.0486\n",
      "      7        0.6807       \u001b[32m0.7188\u001b[0m        \u001b[35m0.6666\u001b[0m     +  0.0455\n",
      "      8        \u001b[36m0.6724\u001b[0m       \u001b[32m0.7500\u001b[0m        0.6709        0.0459\n",
      "      9        \u001b[36m0.6649\u001b[0m       0.7422        \u001b[35m0.6661\u001b[0m     +  0.0467\n",
      "     10        0.6723       0.7266        0.7268        0.0447\n",
      "CPU times: user 778 ms, sys: 174 ms, total: 953 ms\n",
      "Wall time: 993 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dense0): Linear(in_features=11, out_features=66, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (dense1): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (1): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (2): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (3): Linear(in_features=66, out_features=66, bias=True)\n",
       "      (4): Linear(in_features=66, out_features=66, bias=True)\n",
       "    )\n",
       "    (output): Linear(in_features=66, out_features=2, bias=True)\n",
       "    (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimal.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c3a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LoadInitState\n",
    "\n",
    "# load_state = LoadInitState(cp)\n",
    "cp2 = Checkpoint(dirname=f\"{checkpoints_dir}/cp2\")\n",
    "\n",
    "optimal2 = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=7,\n",
    "    module__num_units=66,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp2],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp2] = optimal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c9d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7567\u001b[0m       \u001b[32m0.7188\u001b[0m        \u001b[35m0.6974\u001b[0m     +  0.0541\n",
      "      2        \u001b[36m0.7026\u001b[0m       0.7109        0.7229        0.0506\n",
      "      3        \u001b[36m0.6958\u001b[0m       \u001b[32m0.7266\u001b[0m        \u001b[35m0.6961\u001b[0m     +  0.0533\n",
      "      4        0.6993       \u001b[32m0.7344\u001b[0m        \u001b[35m0.6742\u001b[0m     +  0.0511\n",
      "      5        \u001b[36m0.6912\u001b[0m       \u001b[32m0.7422\u001b[0m        \u001b[35m0.6618\u001b[0m     +  0.0526\n",
      "      6        \u001b[36m0.6862\u001b[0m       0.7266        0.6728        0.0523\n",
      "      7        \u001b[36m0.6830\u001b[0m       0.7266        0.6795        0.0459\n",
      "      8        \u001b[36m0.6782\u001b[0m       0.7344        0.6704        0.0479\n",
      "      9        \u001b[36m0.6749\u001b[0m       0.7266        0.6724        0.0477\n",
      "     10        \u001b[36m0.6719\u001b[0m       0.7266        0.6687        0.0476\n",
      "CPU times: user 507 ms, sys: 38.4 ms, total: 545 ms\n",
      "Wall time: 532 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = optimal2.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd9fe2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_state = LoadInitState(cp2)\n",
    "cp3 = Checkpoint(dirname=f\"{checkpoints_dir}/cp3\")\n",
    "\n",
    "optimal3 = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.001,\n",
    "    batch_size=20,\n",
    "    module__depth=6,\n",
    "    module__num_units=80,\n",
    "    module__dropout=0.4,\n",
    "    device=device,\n",
    "    callbacks=[cp3],\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "models[cp3] = optimal3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785a099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  ------------  -----------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7863\u001b[0m       \u001b[32m0.7500\u001b[0m        \u001b[35m0.6352\u001b[0m     +  0.0507\n",
      "      2        \u001b[36m0.7045\u001b[0m       0.6484        0.6816        0.0528\n",
      "      3        \u001b[36m0.6982\u001b[0m       0.4531        0.8045        0.0485\n",
      "      4        \u001b[36m0.6894\u001b[0m       0.6172        0.6710        0.0465\n",
      "      5        \u001b[36m0.6822\u001b[0m       0.7266        0.6908        0.0516\n",
      "      6        0.6850       0.7266        0.6836        0.0521\n",
      "      7        0.6838       0.7266        0.7210        0.0491\n",
      "      8        \u001b[36m0.6757\u001b[0m       0.7422        0.6679        0.0463\n",
      "      9        \u001b[36m0.6711\u001b[0m       0.7500        0.6626        0.0518\n",
      "     10        0.6744       0.7500        0.6556        0.0502\n",
      "CPU times: user 457 ms, sys: 76.8 ms, total: 534 ms\n",
      "Wall time: 521 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = optimal3.fit(X[:640], y[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b46e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Re-initializing module because the following parameters were re-set: depth, dropout, num_units.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "CPU times: user 35.6 ms, sys: 0 ns, total: 35.6 ms\n",
      "Wall time: 34.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7969348659003831, './checkpoints/cp3')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "checkpoints = [cp, cp2, cp3]\n",
    "\n",
    "best_f1 = 0\n",
    "best_y_pred = None\n",
    "best_net = None\n",
    "\n",
    "for checkpoint in models:\n",
    "    net = models[checkpoint]\n",
    "    net.initialize()\n",
    "    net.load_params(checkpoint=checkpoint)\n",
    "\n",
    "    y_pred = net.predict(X_test)\n",
    "    f_measure = f1_score(y_test, y_pred)\n",
    "    if f_measure > best_f1:\n",
    "        best_f1 = f_measure\n",
    "        best_net = net\n",
    "        best_cp = checkpoint\n",
    "\n",
    "best_f1, best_cp.dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a664a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 ms, sys: 0 ns, total: 7.8 ms\n",
      "Wall time: 6.62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = best_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "258c05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"Model\": [\"Na誰ve Bayes\", \"SVM\", \"Logistic Regression\", \"Random Forest\"],\n",
    "    \"Accuracy\": [\n",
    "        0.6402439024390244,\n",
    "        0.6524390243902439,\n",
    "        0.6463414634146342,\n",
    "        0.7804878048780488,\n",
    "    ],\n",
    "    \"Precision\": [\n",
    "        0.6518987341772152,\n",
    "        0.6524390243902439,\n",
    "        0.855072463768116,\n",
    "        0.7933884297520661,\n",
    "    ],\n",
    "    \"Recall\": [0.9626168224299065, 1.0, 0.5514018691588785, 0.897196261682243],\n",
    "    \"F1 Score\": [\n",
    "        0.7773584905660378,\n",
    "        0.7896678966789668,\n",
    "        0.6704545454545454,\n",
    "        0.8421052631578947,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8635cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.676829268292683\n",
      "Precision: 0.6753246753246753\n",
      "Recall: 0.9719626168224299\n",
      "F-measure: 0.7969348659003831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "model_name = \"Neural Network\"\n",
    "result[\"Model\"].append(model_name)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f_measure = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f_measure)\n",
    "\n",
    "result[\"Accuracy\"].append(accuracy)\n",
    "result[\"Precision\"].append(precision)\n",
    "result[\"Recall\"].append(recall)\n",
    "result[\"F1 Score\"].append(f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defd70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Na誰ve Bayes',\n",
       "  'SVM',\n",
       "  'Logistic Regression',\n",
       "  'Random Forest',\n",
       "  'Neural Network'],\n",
       " 'Accuracy': [0.6402439024390244,\n",
       "  0.6524390243902439,\n",
       "  0.6463414634146342,\n",
       "  0.7804878048780488,\n",
       "  0.676829268292683],\n",
       " 'Precision': [0.6518987341772152,\n",
       "  0.6524390243902439,\n",
       "  0.855072463768116,\n",
       "  0.7933884297520661,\n",
       "  0.6753246753246753],\n",
       " 'Recall': [0.9626168224299065,\n",
       "  1.0,\n",
       "  0.5514018691588785,\n",
       "  0.897196261682243,\n",
       "  0.9719626168224299],\n",
       " 'F1 Score': [0.7773584905660378,\n",
       "  0.7896678966789668,\n",
       "  0.6704545454545454,\n",
       "  0.8421052631578947,\n",
       "  0.7969348659003831]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eea385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/ico/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Na誰ve Bayes</th>\n",
       "      <td>64.0%</td>\n",
       "      <td>65.2%</td>\n",
       "      <td>96.3%</td>\n",
       "      <td>77.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>65.2%</td>\n",
       "      <td>65.2%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>79.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>64.6%</td>\n",
       "      <td>85.5%</td>\n",
       "      <td>55.1%</td>\n",
       "      <td>67.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>78.0%</td>\n",
       "      <td>79.3%</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>84.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>67.7%</td>\n",
       "      <td>67.5%</td>\n",
       "      <td>97.2%</td>\n",
       "      <td>79.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy Precision  Recall F1 Score\n",
       "Model                                                  \n",
       "Na誰ve Bayes            64.0%     65.2%   96.3%    77.7%\n",
       "SVM                    65.2%     65.2%  100.0%    79.0%\n",
       "Logistic Regression    64.6%     85.5%   55.1%    67.0%\n",
       "Random Forest          78.0%     79.3%   89.7%    84.2%\n",
       "Neural Network         67.7%     67.5%   97.2%    79.7%"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "perf_ds2 = Dataset.from_dict(result).to_pandas()\n",
    "res2 = perf_ds2.set_index(perf_ds2.columns[0]).mul(100)\n",
    "for key in res2.select_dtypes(include=[\"number\"]).columns:\n",
    "    res2[key] = res2[key].apply(\"{:.1f}%\".format)\n",
    "\n",
    "res2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
