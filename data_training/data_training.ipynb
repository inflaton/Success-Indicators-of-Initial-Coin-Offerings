{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3d8313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas\n",
    "%pip install -q matplotlib\n",
    "%pip install -q seaborn\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q keras\n",
    "%pip install -q xgboost\n",
    "%pip install -q catboost\n",
    "%pip install -q lightgbm\n",
    "%pip install -q wordcloud\n",
    "%pip install -q nltk\n",
    "%pip install -q imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a98b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "==================================================\n",
      "Best Parameters:  {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8927038626609443\n",
      "Precision: 0.8709677419354839\n",
      "Recall: 0.9230769230769231\n",
      "F1 Score: 0.8962655601659751\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9012875536480687\n",
      "Precision: 0.8983050847457628\n",
      "Recall: 0.905982905982906\n",
      "F1 Score: 0.902127659574468\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9613733905579399\n",
      "Precision: 0.9908256880733946\n",
      "Recall: 0.9310344827586207\n",
      "F1 Score: 0.96\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8369098712446352\n",
      "Precision: 0.9875\n",
      "Recall: 0.6810344827586207\n",
      "F1 Score: 0.8061224489795918\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8706896551724138\n",
      "Precision: 1.0\n",
      "Recall: 0.7413793103448276\n",
      "F1 Score: 0.8514851485148515\n",
      "\n",
      "Average Accuracy: 0.8925928666568004\n",
      "Average Precision: 0.9495197029509281\n",
      "Average Recall: 0.8365016209843796\n",
      "Average F1 Score: 0.8832001634469773\n",
      "\n",
      "Model: Logistic Regression\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.68211114        nan 0.66496596        nan 0.68296951]\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 10, 'penalty': 'l2'}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.7296137339055794\n",
      "Precision: 0.7547169811320755\n",
      "Recall: 0.6837606837606838\n",
      "F1 Score: 0.7174887892376682\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.7424892703862661\n",
      "Precision: 0.7663551401869159\n",
      "Recall: 0.7008547008547008\n",
      "F1 Score: 0.7321428571428571\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.6995708154506438\n",
      "Precision: 0.75\n",
      "Recall: 0.5948275862068966\n",
      "F1 Score: 0.6634615384615384\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.5836909871244635\n",
      "Precision: 0.6862745098039216\n",
      "Recall: 0.3017241379310345\n",
      "F1 Score: 0.41916167664670656\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.6594827586206896\n",
      "Precision: 0.7936507936507936\n",
      "Recall: 0.43103448275862066\n",
      "F1 Score: 0.5586592178770949\n",
      "\n",
      "Average Accuracy: 0.6829695130975285\n",
      "Average Precision: 0.7501994849547413\n",
      "Average Recall: 0.5424403183023873\n",
      "Average F1 Score: 0.618182815873173\n",
      "\n",
      "Model: Naïve Bayes\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.5278969957081545\n",
      "Precision: 0.5660377358490566\n",
      "Recall: 0.2564102564102564\n",
      "F1 Score: 0.35294117647058826\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.51931330472103\n",
      "Precision: 0.5111111111111111\n",
      "Recall: 0.9829059829059829\n",
      "F1 Score: 0.6725146198830408\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.5236051502145923\n",
      "Precision: 0.5117370892018779\n",
      "Recall: 0.9396551724137931\n",
      "F1 Score: 0.6626139817629179\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.47639484978540775\n",
      "Precision: 0.485\n",
      "Recall: 0.8362068965517241\n",
      "F1 Score: 0.6139240506329113\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.5172413793103449\n",
      "Precision: 0.5088495575221239\n",
      "Recall: 0.9913793103448276\n",
      "F1 Score: 0.672514619883041\n",
      "\n",
      "Average Accuracy: 0.5128903359479058\n",
      "Average Precision: 0.5165470987368339\n",
      "Average Recall: 0.8013115237253169\n",
      "Average F1 Score: 0.5949016897264998\n",
      "\n",
      "Model: SVM\n",
      "==================================================\n",
      "Best Parameters:  {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8497854077253219\n",
      "Precision: 0.7697368421052632\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8698884758364311\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.8125\n",
      "Recall: 1.0\n",
      "F1 Score: 0.896551724137931\n",
      "\n",
      "Average Accuracy: 0.866952789699571\n",
      "Average Precision: 0.7911184210526319\n",
      "Average Recall: 1.0\n",
      "Average F1 Score: 0.8832200999871809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Read the data into a dataframe\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df[\n",
    "    [\n",
    "        \"total_direct_mentions\",\n",
    "        \"total_indirect_mentions\",\n",
    "        \"total_likes\",\n",
    "        \"total_retweets\",\n",
    "        \"total_project_followers\",\n",
    "        \"total_indirect_followers\",\n",
    "        \"total_positive_direct_mentions\",\n",
    "        \"total_negative_direct_mentions\",\n",
    "        \"total_positive_indirect_mentions\",\n",
    "        \"total_negative_indirect_mentions\",\n",
    "        \"soft_cap\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"ico_success\"]\n",
    "\n",
    "# Perform Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Define the parameter grids for grid search\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "lr_param_grid = {\"C\": [0.1, 1, 10], \"penalty\": [\"l1\", \"l2\"]}\n",
    "\n",
    "nb_param_grid = {}\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    \"kernel\": [\"rbf\"],\n",
    "}\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    \"Random Forest\": (RandomForestClassifier(random_state=42), rf_param_grid),\n",
    "    \"Logistic Regression\": (LogisticRegression(random_state=42), lr_param_grid),\n",
    "    \"Naïve Bayes\": (GaussianNB(), nb_param_grid),\n",
    "    \"SVM\": (SVC(random_state=42), svm_param_grid),\n",
    "}\n",
    "\n",
    "# Perform grid search and cross-validation for each model\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=5, scoring=\"accuracy\"\n",
    "    )\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    # Perform 5-fold cross-validation with the best model\n",
    "    cv_results = cross_validate(\n",
    "        grid_search.best_estimator_,\n",
    "        X_resampled,\n",
    "        y_resampled,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "    )\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    overfitted_folds = 0  # Counter for overfitted folds\n",
    "    for fold_idx, fold_result in enumerate(cv_results[\"test_accuracy\"]):\n",
    "        if fold_result == 1.0:  # Check for overfitted fold\n",
    "            overfitted_folds += 1\n",
    "            continue  # Skip overfitted fold\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"Accuracy: {fold_result}\")\n",
    "        print(f\"Precision: {cv_results['test_precision'][fold_idx]}\")\n",
    "        print(f\"Recall: {cv_results['test_recall'][fold_idx]}\")\n",
    "        print(f\"F1 Score: {cv_results['test_f1'][fold_idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate average results across non-overfitted folds\n",
    "    num_folds = len(cv_results[\"test_accuracy\"])\n",
    "    num_non_overfitted_folds = num_folds - overfitted_folds\n",
    "    avg_accuracy = (\n",
    "        sum(cv_results[\"test_accuracy\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_precision = (\n",
    "        sum(cv_results[\"test_precision\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_recall = (\n",
    "        sum(cv_results[\"test_recall\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_f1 = (\n",
    "        sum(cv_results[\"test_f1\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "\n",
    "    # Print the average results\n",
    "    print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1 Score: {avg_f1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import sklearn.metrics as mt\n",
    "import nltk\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import LinearSVC\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63c8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcb8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, x_test, y_train, y_test):\n",
    "    # Train the model on the training data\n",
    "    now = time.time()\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Training time: \", time.time() - now)\n",
    "\n",
    "    # Evaluate the trained model on the test data\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # test recall and f1-score\n",
    "    test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    test_prec = metrics.precision_score(y_test, y_pred)\n",
    "    test_recall = metrics.recall_score(y_test, y_pred)\n",
    "    test_f1 = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # train recall and f1-score\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    train_acc = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    train_prec = metrics.precision_score(y_train, y_pred_train)\n",
    "    train_recall = metrics.recall_score(y_train, y_pred_train)\n",
    "    train_f1 = metrics.f1_score(y_train, y_pred_train, average=\"weighted\")\n",
    "\n",
    "    # print the metrics\n",
    "    print(\"Test accuracy: \", test_acc)\n",
    "    print(\"Test precision: \", test_prec)\n",
    "    print(\"Test recall: \", test_recall)\n",
    "    print(\"Test f1-score: \", test_f1)\n",
    "    print(\"Train accuracy: \", train_acc)\n",
    "    print(\"Train precision: \", train_prec)\n",
    "    print(\"Train recall: \", train_recall)\n",
    "    print(\"Train f1-score: \", train_f1)\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be82a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "Training time:  0.1605207920074463\n",
      "Test accuracy:  0.7647058823529411\n",
      "Test precision:  0.7844311377245509\n",
      "Test recall:  0.916083916083916\n",
      "Test f1-score:  0.7450025171358865\n",
      "Train accuracy:  1.0\n",
      "Train precision:  1.0\n",
      "Train recall:  1.0\n",
      "Train f1-score:  1.0\n",
      "Adaboost_GNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.05511021614074707\n",
      "Test accuracy:  0.29411764705882354\n",
      "Test precision:  0.0\n",
      "Test recall:  0.0\n",
      "Test f1-score:  0.1359180035650624\n",
      "Train accuracy:  0.2818627450980392\n",
      "Train precision:  1.0\n",
      "Train recall:  0.010135135135135136\n",
      "Train f1-score:  0.13349488116860375\n",
      "XGBboost\n",
      "Training time:  0.8093397617340088\n",
      "Test accuracy:  0.75\n",
      "Test precision:  0.7674418604651163\n",
      "Test recall:  0.9230769230769231\n",
      "Test f1-score:  0.7225294418842807\n",
      "Train accuracy:  0.9191176470588235\n",
      "Train precision:  0.9096573208722741\n",
      "Train recall:  0.9864864864864865\n",
      "Train f1-score:  0.9156757186638694\n",
      "Gradient Boosting\n",
      "Training time:  0.23137974739074707\n",
      "Test accuracy:  0.7573529411764706\n",
      "Test precision:  0.7791044776119403\n",
      "Test recall:  0.9125874125874126\n",
      "Test f1-score:  0.7364395479484993\n",
      "Train accuracy:  0.9877450980392157\n",
      "Train precision:  0.9833887043189369\n",
      "Train recall:  1.0\n",
      "Train f1-score:  0.9876565205830387\n",
      "CatBoost\n",
      "Training time:  0.2796640396118164\n",
      "Test accuracy:  0.7941176470588235\n",
      "Test precision:  0.8023952095808383\n",
      "Test recall:  0.9370629370629371\n",
      "Test f1-score:  0.7768772024939009\n",
      "Train accuracy:  0.9779411764705882\n",
      "Train precision:  0.9704918032786886\n",
      "Train recall:  1.0\n",
      "Train f1-score:  0.9776446460641294\n",
      "Random Forest\n",
      "Training time:  0.15347886085510254\n",
      "Test accuracy:  0.7745098039215687\n",
      "Test precision:  0.793939393939394\n",
      "Test recall:  0.916083916083916\n",
      "Test f1-score:  0.7577591036414566\n",
      "Train accuracy:  1.0\n",
      "Train precision:  1.0\n",
      "Train recall:  1.0\n",
      "Train f1-score:  1.0\n",
      "StackingClassifier\n",
      "Training time:  0.058608293533325195\n",
      "Test accuracy:  0.7009803921568627\n",
      "Test precision:  0.7009803921568627\n",
      "Test recall:  1.0\n",
      "Test f1-score:  0.5777532915183364\n",
      "Train accuracy:  0.7254901960784313\n",
      "Train precision:  0.7254901960784313\n",
      "Train recall:  1.0\n",
      "Train f1-score:  0.6100713012477718\n",
      "LightGBM\n",
      "[LightGBM] [Info] Number of positive: 296, number of negative: 112\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1505\n",
      "[LightGBM] [Info] Number of data points in the train set: 408, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.725490 -> initscore=0.971861\n",
      "[LightGBM] [Info] Start training from score 0.971861\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training time:  0.09055328369140625\n",
      "Test accuracy:  0.75\n",
      "Test precision:  0.7822085889570553\n",
      "Test recall:  0.8916083916083916\n",
      "Test f1-score:  0.7336601307189543\n",
      "Train accuracy:  0.9975490196078431\n",
      "Train precision:  0.9966329966329966\n",
      "Train recall:  1.0\n",
      "Train f1-score:  0.9975455907300345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        algorithm=\"SAMME\",\n",
    "        base_estimator=RandomForestClassifier(random_state=123),\n",
    "        learning_rate=1,\n",
    "        n_estimators=10,\n",
    "        random_state=123,\n",
    "    ),\n",
    "    \"Adaboost_GNB\": AdaBoostClassifier(\n",
    "        base_estimator=GaussianNB(var_smoothing=0.012328467394420659), random_state=42\n",
    "    ),\n",
    "    \"XGBboost\": XGBClassifier(\n",
    "        random_state=42, learning_rate=0.03, max_depth=3, n_estimators=300, reg_lambda=2\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        random_state=42,\n",
    "        iterations=200,\n",
    "        depth=4,\n",
    "        loss_function=\"Logloss\",\n",
    "        l2_leaf_reg=1e-20,\n",
    "        leaf_estimation_iterations=10,\n",
    "        logging_level=\"Silent\",\n",
    "        learning_rate=0.03,\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"StackingClassifier\": StackingClassifier(\n",
    "        estimators=[\n",
    "            (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "            (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    models[model] = train_and_evaluate_model(\n",
    "        models[model], X_train, X_test, y_train, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2a1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoost\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'learning_rate': 0.1, 'n_estimators': 10}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8927038626609443\n",
      "Precision: 0.8709677419354839\n",
      "Recall: 0.9230769230769231\n",
      "F1 Score: 0.8962655601659751\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9012875536480687\n",
      "Precision: 0.9051724137931034\n",
      "Recall: 0.8974358974358975\n",
      "F1 Score: 0.9012875536480686\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9656652360515021\n",
      "Precision: 1.0\n",
      "Recall: 0.9310344827586207\n",
      "F1 Score: 0.9642857142857143\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8283261802575107\n",
      "Precision: 1.0\n",
      "Recall: 0.6551724137931034\n",
      "F1 Score: 0.7916666666666666\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.875\n",
      "Precision: 1.0\n",
      "Recall: 0.75\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "Average Accuracy: 0.8925965665236053\n",
      "Average Precision: 0.9552280311457174\n",
      "Average Recall: 0.8313439434129088\n",
      "Average F1 Score: 0.8821296703818563\n",
      "\n",
      "Model: Adaboost_GNB\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/inflaton/miniconda3/envs/cs701/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.51931330472103\n",
      "Precision: 0.5109170305676856\n",
      "Recall: 1.0\n",
      "F1 Score: 0.676300578034682\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.4892703862660944\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.463519313304721\n",
      "Precision: 0.4117647058823529\n",
      "Recall: 0.1810344827586207\n",
      "F1 Score: 0.25149700598802394\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.5150214592274678\n",
      "Precision: 0.5384615384615384\n",
      "Recall: 0.1810344827586207\n",
      "F1 Score: 0.2709677419354839\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.4870689655172414\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n",
      "Average Accuracy: 0.49483868580731094\n",
      "Average Precision: 0.2922286549823154\n",
      "Average Recall: 0.2724137931034483\n",
      "Average F1 Score: 0.23975306519163797\n",
      "\n",
      "Model: XGBboost\n",
      "==================================================\n",
      "Best Parameters:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, 'reg_lambda': 5}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.8629032258064516\n",
      "Recall: 0.9145299145299145\n",
      "F1 Score: 0.8879668049792532\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8969957081545065\n",
      "Precision: 0.911504424778761\n",
      "Recall: 0.8803418803418803\n",
      "F1 Score: 0.8956521739130435\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9356223175965666\n",
      "Precision: 1.0\n",
      "Recall: 0.8706896551724138\n",
      "F1 Score: 0.9308755760368664\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8283261802575107\n",
      "Precision: 1.0\n",
      "Recall: 0.6551724137931034\n",
      "F1 Score: 0.7916666666666666\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.896551724137931\n",
      "Precision: 1.0\n",
      "Recall: 0.7931034482758621\n",
      "F1 Score: 0.8846153846153846\n",
      "\n",
      "Average Accuracy: 0.888323220364067\n",
      "Average Precision: 0.9548815301170425\n",
      "Average Recall: 0.8227674624226348\n",
      "Average F1 Score: 0.8781553212422428\n",
      "\n",
      "Model: Gradient Boosting\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8283261802575107\n",
      "Precision: 0.7938931297709924\n",
      "Recall: 0.8888888888888888\n",
      "F1 Score: 0.8387096774193549\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8240343347639485\n",
      "Precision: 0.8220338983050848\n",
      "Recall: 0.8290598290598291\n",
      "F1 Score: 0.825531914893617\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.8927038626609443\n",
      "Precision: 0.8956521739130435\n",
      "Recall: 0.8879310344827587\n",
      "F1 Score: 0.8917748917748919\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.7854077253218884\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.6206896551724138\n",
      "F1 Score: 0.7422680412371133\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8275862068965517\n",
      "Precision: 0.8958333333333334\n",
      "Recall: 0.7413793103448276\n",
      "F1 Score: 0.8113207547169813\n",
      "\n",
      "Average Accuracy: 0.8316116619801688\n",
      "Average Precision: 0.8660978916798754\n",
      "Average Recall: 0.7935897435897437\n",
      "Average F1 Score: 0.8219210560083917\n",
      "\n",
      "Model: CatBoost\n",
      "==================================================\n",
      "Best Parameters:  {'depth': 10, 'iterations': 300, 'learning_rate': 0.01}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.835820895522388\n",
      "Recall: 0.9572649572649573\n",
      "F1 Score: 0.8924302788844622\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.927038626609442\n",
      "Precision: 0.9032258064516129\n",
      "Recall: 0.9572649572649573\n",
      "F1 Score: 0.9294605809128631\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9742489270386266\n",
      "Precision: 1.0\n",
      "Recall: 0.9482758620689655\n",
      "F1 Score: 0.9734513274336283\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8626609442060086\n",
      "Precision: 1.0\n",
      "Recall: 0.7241379310344828\n",
      "F1 Score: 0.8400000000000001\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.896551724137931\n",
      "Precision: 1.0\n",
      "Recall: 0.7931034482758621\n",
      "F1 Score: 0.8846153846153846\n",
      "\n",
      "Average Accuracy: 0.9089240787331656\n",
      "Average Precision: 0.9478093403948001\n",
      "Average Recall: 0.8760094311818449\n",
      "Average F1 Score: 0.9039915143692678\n",
      "\n",
      "Model: Random Forest\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8969957081545065\n",
      "Precision: 0.872\n",
      "Recall: 0.9316239316239316\n",
      "F1 Score: 0.9008264462809916\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9098712446351931\n",
      "Precision: 0.9137931034482759\n",
      "Recall: 0.905982905982906\n",
      "F1 Score: 0.9098712446351932\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9570815450643777\n",
      "Precision: 1.0\n",
      "Recall: 0.9137931034482759\n",
      "F1 Score: 0.9549549549549551\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8326180257510729\n",
      "Precision: 1.0\n",
      "Recall: 0.6637931034482759\n",
      "F1 Score: 0.7979274611398964\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8879310344827587\n",
      "Precision: 1.0\n",
      "Recall: 0.7758620689655172\n",
      "F1 Score: 0.8737864077669902\n",
      "\n",
      "Average Accuracy: 0.8968995116175817\n",
      "Average Precision: 0.9571586206896552\n",
      "Average Recall: 0.8382110226937813\n",
      "Average F1 Score: 0.8874733029556052\n",
      "\n",
      "Model: StackingClassifier\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8583690987124464\n",
      "Precision: 0.85\n",
      "Recall: 0.8717948717948718\n",
      "F1 Score: 0.8607594936708861\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8154506437768241\n",
      "Precision: 0.87\n",
      "Recall: 0.7435897435897436\n",
      "F1 Score: 0.8018433179723502\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.8927038626609443\n",
      "Precision: 1.0\n",
      "Recall: 0.7844827586206896\n",
      "F1 Score: 0.8792270531400966\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8326180257510729\n",
      "Precision: 1.0\n",
      "Recall: 0.6637931034482759\n",
      "F1 Score: 0.7979274611398964\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8318965517241379\n",
      "Precision: 1.0\n",
      "Recall: 0.6637931034482759\n",
      "F1 Score: 0.7979274611398964\n",
      "\n",
      "Average Accuracy: 0.8462076365250851\n",
      "Average Precision: 0.944\n",
      "Average Recall: 0.7454907161803713\n",
      "Average F1 Score: 0.8275369574126252\n",
      "\n",
      "Model: LightGBM\n",
      "==================================================\n",
      "[LightGBM] [Info] Number of positive: 465, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2782\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499463 -> initscore=-0.002148\n",
      "[LightGBM] [Info] Start training from score -0.002148\n",
      "[LightGBM] [Info] Number of positive: 465, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499463 -> initscore=-0.002148\n",
      "[LightGBM] [Info] Start training from score -0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 465\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500537 -> initscore=0.002148\n",
      "[LightGBM] [Info] Start training from score 0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 465\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2781\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500537 -> initscore=0.002148\n",
      "[LightGBM] [Info] Start training from score 0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2791\n",
      "[LightGBM] [Info] Number of data points in the train set: 932, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 582, number of negative: 582\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2789\n",
      "[LightGBM] [Info] Number of data points in the train set: 1164, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best Parameters:  {}\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 465, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2782\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499463 -> initscore=-0.002148\n",
      "[LightGBM] [Info] Start training from score -0.002148\n",
      "[LightGBM] [Info] Number of positive: 465, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499463 -> initscore=-0.002148\n",
      "[LightGBM] [Info] Start training from score -0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 465\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2790\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500537 -> initscore=0.002148\n",
      "[LightGBM] [Info] Start training from score 0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 465\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2781\n",
      "[LightGBM] [Info] Number of data points in the train set: 931, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500537 -> initscore=0.002148\n",
      "[LightGBM] [Info] Start training from score 0.002148\n",
      "[LightGBM] [Info] Number of positive: 466, number of negative: 466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2791\n",
      "[LightGBM] [Info] Number of data points in the train set: 932, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.871244635193133\n",
      "Precision: 0.8536585365853658\n",
      "Recall: 0.8974358974358975\n",
      "F1 Score: 0.875\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.8879310344827587\n",
      "Recall: 0.8803418803418803\n",
      "F1 Score: 0.8841201716738197\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9613733905579399\n",
      "Precision: 1.0\n",
      "Recall: 0.9224137931034483\n",
      "F1 Score: 0.9596412556053812\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8068669527896996\n",
      "Precision: 1.0\n",
      "Recall: 0.6120689655172413\n",
      "F1 Score: 0.7593582887700534\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8922413793103449\n",
      "Precision: 1.0\n",
      "Recall: 0.7844827586206896\n",
      "F1 Score: 0.8792270531400966\n",
      "\n",
      "Average Accuracy: 0.8831693059049874\n",
      "Average Precision: 0.9483179142136249\n",
      "Average Recall: 0.8193486590038314\n",
      "Average F1 Score: 0.8714693538378702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define the models with their initial parameter settings\n",
    "models = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        algorithm=\"SAMME\",\n",
    "        base_estimator=RandomForestClassifier(random_state=123),\n",
    "        learning_rate=1,\n",
    "        n_estimators=10,\n",
    "        random_state=123,\n",
    "    ),\n",
    "    \"Adaboost_GNB\": AdaBoostClassifier(\n",
    "        base_estimator=GaussianNB(var_smoothing=0.012328467394420659), random_state=42\n",
    "    ),\n",
    "    \"XGBboost\": XGBClassifier(\n",
    "        random_state=42, learning_rate=0.03, max_depth=3, n_estimators=300, reg_lambda=2\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        random_state=42,\n",
    "        iterations=200,\n",
    "        depth=4,\n",
    "        loss_function=\"Logloss\",\n",
    "        l2_leaf_reg=1e-20,\n",
    "        leaf_estimation_iterations=10,\n",
    "        logging_level=\"Silent\",\n",
    "        learning_rate=0.03,\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"StackingClassifier\": StackingClassifier(\n",
    "        estimators=[\n",
    "            (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "            (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Define the parameter grids for grid search\n",
    "param_grids = {\n",
    "    \"AdaBoost\": {\"n_estimators\": [10, 50, 100], \"learning_rate\": [0.1, 0.5, 1.0]},\n",
    "    \"Adaboost_GNB\": {},\n",
    "    \"XGBboost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "        \"reg_lambda\": [1, 2, 5],\n",
    "    },\n",
    "    \"Gradient Boosting\": {},\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "        \"depth\": [3, 5, 10],\n",
    "    },\n",
    "    \"Random Forest\": {},\n",
    "    \"StackingClassifier\": {},\n",
    "    \"LightGBM\": {},\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get the parameter grid for the current model\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=5, scoring=\"accuracy\"\n",
    "    )\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    # Perform 5-fold cross-validation with the best model\n",
    "    cv_results = cross_validate(\n",
    "        grid_search.best_estimator_,\n",
    "        X_resampled,\n",
    "        y_resampled,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "    )\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    overfitted_folds = 0  # Counter for overfitted folds\n",
    "    for fold_idx, fold_result in enumerate(cv_results[\"test_accuracy\"]):\n",
    "        if fold_result == 1.0:  # Check for overfitted fold\n",
    "            overfitted_folds += 1\n",
    "            continue  # Skip overfitted fold\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"Accuracy: {fold_result}\")\n",
    "        print(f\"Precision: {cv_results['test_precision'][fold_idx]}\")\n",
    "        print(f\"Recall: {cv_results['test_recall'][fold_idx]}\")\n",
    "        print(f\"F1 Score: {cv_results['test_f1'][fold_idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate average results across non-overfitted folds\n",
    "    num_folds = len(cv_results[\"test_accuracy\"])\n",
    "    num_non_overfitted_folds = num_folds - overfitted_folds\n",
    "    avg_accuracy = (\n",
    "        sum(cv_results[\"test_accuracy\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_precision = (\n",
    "        sum(cv_results[\"test_precision\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_recall = (\n",
    "        sum(cv_results[\"test_recall\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_f1 = (\n",
    "        sum(cv_results[\"test_f1\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "\n",
    "    # Print the average results\n",
    "    print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1 Score: {avg_f1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b722f3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBboost\n",
      "==================================================\n",
      "Best Parameters:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, 'reg_lambda': 5}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.8629032258064516\n",
      "Recall: 0.9145299145299145\n",
      "F1 Score: 0.8879668049792532\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8969957081545065\n",
      "Precision: 0.911504424778761\n",
      "Recall: 0.8803418803418803\n",
      "F1 Score: 0.8956521739130435\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9356223175965666\n",
      "Precision: 1.0\n",
      "Recall: 0.8706896551724138\n",
      "F1 Score: 0.9308755760368664\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8283261802575107\n",
      "Precision: 1.0\n",
      "Recall: 0.6551724137931034\n",
      "F1 Score: 0.7916666666666666\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.896551724137931\n",
      "Precision: 1.0\n",
      "Recall: 0.7931034482758621\n",
      "F1 Score: 0.8846153846153846\n",
      "\n",
      "Average Accuracy: 0.888323220364067\n",
      "Average Precision: 0.9548815301170425\n",
      "Average Recall: 0.8227674624226348\n",
      "Average F1 Score: 0.8781553212422428\n",
      "\n",
      "Model: CatBoost\n",
      "==================================================\n",
      "Best Parameters:  {'depth': 10, 'iterations': 300, 'learning_rate': 0.01}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8841201716738197\n",
      "Precision: 0.835820895522388\n",
      "Recall: 0.9572649572649573\n",
      "F1 Score: 0.8924302788844622\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.927038626609442\n",
      "Precision: 0.9032258064516129\n",
      "Recall: 0.9572649572649573\n",
      "F1 Score: 0.9294605809128631\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9742489270386266\n",
      "Precision: 1.0\n",
      "Recall: 0.9482758620689655\n",
      "F1 Score: 0.9734513274336283\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8626609442060086\n",
      "Precision: 1.0\n",
      "Recall: 0.7241379310344828\n",
      "F1 Score: 0.8400000000000001\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.896551724137931\n",
      "Precision: 1.0\n",
      "Recall: 0.7931034482758621\n",
      "F1 Score: 0.8846153846153846\n",
      "\n",
      "Average Accuracy: 0.9089240787331656\n",
      "Average Precision: 0.9478093403948001\n",
      "Average Recall: 0.8760094311818449\n",
      "Average F1 Score: 0.9039915143692678\n",
      "\n",
      "Model: Random Forest\n",
      "==================================================\n",
      "Best Parameters:  {}\n",
      "\n",
      "Cross-Validation Results:\n",
      "==================================================\n",
      "Fold 1:\n",
      "Accuracy: 0.8969957081545065\n",
      "Precision: 0.872\n",
      "Recall: 0.9316239316239316\n",
      "F1 Score: 0.9008264462809916\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.9098712446351931\n",
      "Precision: 0.9137931034482759\n",
      "Recall: 0.905982905982906\n",
      "F1 Score: 0.9098712446351932\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.9570815450643777\n",
      "Precision: 1.0\n",
      "Recall: 0.9137931034482759\n",
      "F1 Score: 0.9549549549549551\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8326180257510729\n",
      "Precision: 1.0\n",
      "Recall: 0.6637931034482759\n",
      "F1 Score: 0.7979274611398964\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.8879310344827587\n",
      "Precision: 1.0\n",
      "Recall: 0.7758620689655172\n",
      "F1 Score: 0.8737864077669902\n",
      "\n",
      "Average Accuracy: 0.8968995116175817\n",
      "Average Precision: 0.9571586206896552\n",
      "Average Recall: 0.8382110226937813\n",
      "Average F1 Score: 0.8874733029556052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"XGBboost\": XGBClassifier(\n",
    "        random_state=42, learning_rate=0.03, max_depth=3, n_estimators=300, reg_lambda=2\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        random_state=42,\n",
    "        iterations=200,\n",
    "        depth=4,\n",
    "        loss_function=\"Logloss\",\n",
    "        l2_leaf_reg=1e-20,\n",
    "        leaf_estimation_iterations=10,\n",
    "        logging_level=\"Silent\",\n",
    "        learning_rate=0.03,\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Define the parameter grids for grid search\n",
    "param_grids = {\n",
    "    \"XGBboost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "        \"reg_lambda\": [1, 2, 5],\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.1],\n",
    "        \"depth\": [3, 5, 10],\n",
    "    },\n",
    "    \"Random Forest\": {},\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get the parameter grid for the current model\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=5, scoring=\"accuracy\"\n",
    "    )\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Print the best parameters and the corresponding score\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "    # Perform 5-fold cross-validation with the best model\n",
    "    cv_results = cross_validate(\n",
    "        grid_search.best_estimator_,\n",
    "        X_resampled,\n",
    "        y_resampled,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "    )\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    overfitted_folds = 0  # Counter for overfitted folds\n",
    "    for fold_idx, fold_result in enumerate(cv_results[\"test_accuracy\"]):\n",
    "        if fold_result == 1.0:  # Check for overfitted fold\n",
    "            overfitted_folds += 1\n",
    "            continue  # Skip overfitted fold\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"Accuracy: {fold_result}\")\n",
    "        print(f\"Precision: {cv_results['test_precision'][fold_idx]}\")\n",
    "        print(f\"Recall: {cv_results['test_recall'][fold_idx]}\")\n",
    "        print(f\"F1 Score: {cv_results['test_f1'][fold_idx]}\")\n",
    "        print()\n",
    "\n",
    "    # Calculate average results across non-overfitted folds\n",
    "    num_folds = len(cv_results[\"test_accuracy\"])\n",
    "    num_non_overfitted_folds = num_folds - overfitted_folds\n",
    "    avg_accuracy = (\n",
    "        sum(cv_results[\"test_accuracy\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_precision = (\n",
    "        sum(cv_results[\"test_precision\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_recall = (\n",
    "        sum(cv_results[\"test_recall\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "    avg_f1 = (\n",
    "        sum(cv_results[\"test_f1\"]) - (overfitted_folds * 1)\n",
    "    ) / num_non_overfitted_folds\n",
    "\n",
    "    # Print the average results\n",
    "    print(f\"Average Accuracy: {avg_accuracy}\")\n",
    "    print(f\"Average Precision: {avg_precision}\")\n",
    "    print(f\"Average Recall: {avg_recall}\")\n",
    "    print(f\"Average F1 Score: {avg_f1}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
